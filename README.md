# Language Detection Challenge: Word2Vec Baseline

## Project Context

This repository serves as a technical assessment for research interns.

Previous iterations of this problem solved language detection using sparse vector representations (TF-IDF) paired with Naive Bayes or Logistic Regression. While effective for simple keywords, sparse representations often fail to capture the semantic context of words.

This project shifts the focus to **Word Embeddings (Dense Vectors)**. By using Word2Vec, we aim to capture the semantic relationships between words more effectively than frequency-based methods.

## The Baseline Implementation

The provided script (`script.py`) establishes a baseline accuracy that you are tasked with improving. It implements a pipeline using **Gensim** and **Scikit-Learn**:

1.  **Preprocessing:** Tokenization using Gensim's simple preprocessor.
2.  **Embedding:** A Word2Vec model is trained *from scratch* on the training data (not pre-trained).
      * *Parameters:* Vector size 100, Window size 5.
3.  **Vectorization:** Document vectors are generated by calculating the **mean** of all valid word vectors in a given text string.
4.  **Classification:** A Random Forest Classifier is trained on the averaged document vectors.

## The Challenge

**Your Goal:** Beat the classification report metrics (Precision, Recall, F1-Score) of this baseline implementation.

You may utilize any method you deem appropriate, provided you can explain your reasoning. Suggested avenues for improvement include:

  * **Better Embeddings:** The current model trains embeddings from scratch on a small dataset. Consider using pre-trained embeddings (FastText, GloVe, or Google News Word2Vec) to capture better semantic meaning.
  * **Better Vectorization:** Averaging word vectors loses word order information. Consider concatenation, weighting by TF-IDF, or using Doc2Vec.
  * **Model Selection:** Random Forest is robust, but other algorithms (SVM, XGBoost, or a simple Neural Network) might handle dense vectors better.
  * **Data Cleaning:** The current preprocessing is minimal.

## Setup and Usage

### Prerequisites

Ensure you have the following libraries installed. You can install them via pip:

```bash
pip install pandas gensim scikit-learn numpy
```

### Dataset

The script requires a file named `Language Detection.csv` in the root directory. Ensure this file contains at least two columns: `Text` and `Language`.

### Running the Baseline

Run the script to generate the baseline classification report:

```bash
python script.py
```

## Evaluation

Success is measured by a demonstrable increase in the weighted average F1-score compared to the output of the provided `script.py`.

Please submit your improved code along with a short summary explaining:

1.  The approach you took.
2.  Why you chose that approach over the baseline.
3.  The final metrics achieved.
